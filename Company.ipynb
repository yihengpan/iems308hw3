{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '2013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_files(path):\n",
    "    criteria = os.listdir(path)\n",
    "    if len (criteria) == 0:\n",
    "        raise Exception ('This is an empty file')\n",
    "    else:\n",
    "        files = glob.glob(path + '/*')\n",
    "        frame = []\n",
    "        for f in files:\n",
    "            text = open(f, 'r', encoding = 'latin-1').read()\n",
    "            frame.append(text)\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    }
   ],
   "source": [
    "c2013 = read_all_files(path1)\n",
    "print(len(c2013))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if Capital Name is stopword\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def check_stopwords(word):\n",
    "    if word.lower() not in stop_words:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683290\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenize\n",
    "\n",
    "sentence13 = []\n",
    "for item in c2013:\n",
    "    sentence13.extend(sent_tokenize(item))\n",
    "    \n",
    "print(len(sentence13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_token(corpus):\n",
    "    sen_corpus = []\n",
    "    for file in corpus:\n",
    "        token_file = sent_tokenize(file)\n",
    "        sen_corpus.append(token_file)\n",
    "    return sen_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    }
   ],
   "source": [
    "sen_corpus = sent_token(c2013)\n",
    "print(len(sen_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download our train data\n",
    "company_path = 'all/company.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_company(name):\n",
    "     \n",
    "    name = name.strip().split(',')\n",
    "    name = filter(None, name)\n",
    "    name = list(map(lambda x: x.strip(),name))\n",
    "    \n",
    "    name = word_token(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_file = open(company_path, 'r', encoding = 'latin-1').readlines()\n",
    "company_names = map(lambda x: process_company(x), ceo_file)\n",
    "company_names = list(set(company_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "split0 = int(0.6* len(sen_corpus))\n",
    "split1 = int(0.8* len(sen_corpus))\n",
    "\n",
    "training_set = sen_corpus[0:split0]\n",
    "validation_set = sen_corpus[split0:split1]\n",
    "testing_set = sen_corpus[split1:]\n",
    "\n",
    "assert len(sen_corpus) == len(training_set) + len(validation_set) + len(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names(article):\n",
    "    \n",
    "    rep = '(([A-z][a-z]+) ([A-z][a-z]+)*)'\n",
    "    pattern = re.compile(rep)\n",
    "    \n",
    "    names = map(lambda x: re.findall(pattern,x ), article)\n",
    "    names = filter(None, names)\n",
    "    names = itertools.chain(*names)\n",
    "    \n",
    "    names = filter(check_stopwords, names)\n",
    "    \n",
    "    names = set(names)\n",
    "    \n",
    "    return names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = set(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(article, names):\n",
    "    \n",
    "    def check_in_sentence(name, sentence):\n",
    "        return ((' '+ name +' ' in sentence) or (name + ' ' == sentence[:len(name)+1] or (' ' + name == sentence[-len(name)+1:])))\n",
    " \n",
    "    vectors = []\n",
    "    features = {}\n",
    "    for name in names:\n",
    "        sentences = filter(lambda x: check_in_sentence(name,x), article)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            \n",
    "            features['is_company'] = int(name in company)\n",
    "            first, last = name.split()\n",
    "            features['first'] = len(first)\n",
    "            word_list1 = []\n",
    "            word_list2 = []\n",
    "            word_length = len(name)\n",
    "            first_index = s.index(first)\n",
    "            \n",
    "            word_list1 = nltk.pos_tag(name[first_index - 1])\n",
    "            word_list2 = nltk.pos_tag(name[first_index + word_length + 1])\n",
    "            \n",
    "            s = word_tokenize(sentence)\n",
    "            \n",
    "            features['front_pos'] = word_list1[1]\n",
    "            features['last_pos'] = word_list2[1]\n",
    "            if sentence.count('Cor') > 0 or sentence.count('Company') > 0 or sentence.count('Group') or sentence.count('Ltd') or sentence.count('Co'):\n",
    "                features['specific'] = 1\n",
    "            else:\n",
    "                features['specific'] = 0\n",
    "            vectors.append(features)\n",
    "    return vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b2bac4912fa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-250f0cfc5aa0>\u001b[0m in \u001b[0;36mfind_names\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_stopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ed93f006182c>\u001b[0m in \u001b[0;36mcheck_stopwords\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for i, article in enumerate(training_set):\n",
    "    names = find_names(article)\n",
    "    vectors = get_feature(article, names)\n",
    "    train_data.extend(vectors)\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['is_company'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = df[df.is_company == 1]\n",
    "df_negative = df[df.is_company == 0]\n",
    "\n",
    "n_per_class = 10*len(df_positive)\n",
    "\n",
    "df_positive = resample(df_positive, replace = True, n_samples = n_per_class)\n",
    "df_negative = resample(df_negative, replace = False, n_samples = n_per_class)\n",
    "df1 = pd.concat([df_positive, df_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 20)\n",
    "col = [c for c in df1.columns if c not in ('is_ceo','name')]\n",
    "rfc.fit(df1[col], df1['is_ceo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
